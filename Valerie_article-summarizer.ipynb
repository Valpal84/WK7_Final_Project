{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Valerie Johnson\n",
    "### Week 7-Final Project Repo: https://github.com/Valpal84/WK7_Final_Project\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing/importing necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ ------------\n",
      "annotated-types    0.7.0\n",
      "asttokens          3.0.0\n",
      "beautifulsoup4     4.12.3\n",
      "blis               1.0.1\n",
      "bs4                0.0.2\n",
      "catalogue          2.0.10\n",
      "certifi            2024.8.30\n",
      "charset-normalizer 3.4.0\n",
      "click              8.1.7\n",
      "cloudpathlib       0.20.0\n",
      "colorama           0.4.6\n",
      "comm               0.2.2\n",
      "confection         0.1.5\n",
      "contourpy          1.3.1\n",
      "cycler             0.12.1\n",
      "cymem              2.0.10\n",
      "debugpy            1.8.9\n",
      "decorator          5.1.1\n",
      "executing          2.1.0\n",
      "fonttools          4.55.0\n",
      "html5lib           1.1\n",
      "idna               3.10\n",
      "ipykernel          6.29.5\n",
      "ipython            8.30.0\n",
      "jedi               0.19.2\n",
      "Jinja2             3.1.4\n",
      "joblib             1.4.2\n",
      "jupyter_client     8.6.3\n",
      "jupyter_core       5.7.2\n",
      "kiwisolver         1.4.7\n",
      "langcodes          3.5.0\n",
      "language_data      1.3.0\n",
      "marisa-trie        1.2.1\n",
      "markdown-it-py     3.0.0\n",
      "MarkupSafe         3.0.2\n",
      "matplotlib         3.9.3\n",
      "matplotlib-inline  0.1.7\n",
      "mdurl              0.1.2\n",
      "murmurhash         1.0.11\n",
      "nest-asyncio       1.6.0\n",
      "nltk               3.9.1\n",
      "numpy              2.0.2\n",
      "packaging          24.2\n",
      "parso              0.8.4\n",
      "pillow             11.0.0\n",
      "pip                23.2.1\n",
      "platformdirs       4.3.6\n",
      "preshed            3.0.9\n",
      "prompt_toolkit     3.0.48\n",
      "psutil             6.1.0\n",
      "pure_eval          0.2.3\n",
      "pydantic           2.10.2\n",
      "pydantic_core      2.27.1\n",
      "Pygments           2.18.0\n",
      "pyparsing          3.2.0\n",
      "python-dateutil    2.9.0.post0\n",
      "pywin32            308\n",
      "pyzmq              26.2.0\n",
      "regex              2024.11.6\n",
      "requests           2.32.3\n",
      "rich               13.9.4\n",
      "setuptools         65.5.0\n",
      "shellingham        1.5.4\n",
      "six                1.16.0\n",
      "smart-open         7.0.5\n",
      "soupsieve          2.6\n",
      "spacy              3.8.2\n",
      "spacy-legacy       3.0.12\n",
      "spacy-loggers      1.0.5\n",
      "spacytextblob      5.0.0\n",
      "srsly              2.4.8\n",
      "stack-data         0.6.3\n",
      "textblob           0.18.0.post0\n",
      "thinc              8.3.2\n",
      "tornado            6.4.2\n",
      "tqdm               4.67.1\n",
      "traitlets          5.14.3\n",
      "typer              0.14.0\n",
      "typing_extensions  4.12.2\n",
      "urllib3            2.2.3\n",
      "wasabi             1.1.3\n",
      "wcwidth            0.2.13\n",
      "weasel             0.4.1\n",
      "webencodings       0.5.1\n",
      "wrapt              1.17.0\n",
      "All prereqs installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Create and activate a Python virtual environment. \n",
    "# Before starting the project, try all these imports FIRST\n",
    "# Address any errors you get running this code cell \n",
    "# by installing the necessary packages into your active Python environment.\n",
    "# Try to resolve issues using your materials and the web.\n",
    "# If that doesn't work, ask for help in the discussion forums.\n",
    "# You can't complete the exercises until you import these - start early! \n",
    "# We also import pickle and Counter (included in the Python Standard Library).\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip list\n",
    "print('All prereqs installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch successful!\n",
      "Content-Type: application/json; charset=utf-8\n",
      "HTML file created: song_lyrics.html\n",
      "HTML loaded from pickle: \n",
      "    <!DOCTYPE html>\n",
      "    <html>\n",
      "    <head>\n",
      "        <title>Song Lyrics: Imagine Dragons - Demons</tit ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pickle\n",
    "\n",
    "# Get the necessary article (lyrics in this case)\n",
    "response = requests.get('https://api.lyrics.ovh/v1/Imagine%20Dragons/Demons')\n",
    "\n",
    "# Check status of the fetch\n",
    "if response.status_code == 200:\n",
    "    print(\"Fetch successful!\")\n",
    "    print(\"Content-Type:\", response.headers['content-type'])\n",
    "\n",
    "    # Get the lyrics text\n",
    "    lyrics_data = response.json()  # The API returns JSON\n",
    "    lyrics = lyrics_data.get(\"lyrics\", \"No lyrics found.\")  # Extract lyrics\n",
    "\n",
    "    # Create a simple HTML structure\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Song Lyrics: Imagine Dragons - Demons</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Imagine Dragons - Demons</h1>\n",
    "        <pre>{lyrics}</pre>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the HTML to a file\n",
    "    with open('song_lyrics.html', 'w', encoding='utf-8') as html_file:\n",
    "        html_file.write(html_content)\n",
    "    print(\"HTML file created: song_lyrics.html\")\n",
    "\n",
    "    # Create a pickle file for HTML content\n",
    "    with open('song_html.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(html_content, pickle_file)\n",
    "\n",
    "    # Read back the pickle file\n",
    "    with open('song_html.pkl', 'rb') as pickle_file:\n",
    "        loaded_html = pickle.load(pickle_file)\n",
    "        print(\"HTML loaded from pickle:\", loaded_html[:100], \"...\")  # Print a snippet of the HTML\n",
    "else:\n",
    "    print(f\"Failed to fetch lyrics: {response.status_code}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed lyrics text: When the days are cold. . And the cards all fold. . And the saints we see. . Are all made of gold. . ...\n",
      "The sentiment polarity of the lyrics is: -0.19 which is a Negative polarity score\n",
      "Number of sentences in the lyrics: 149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Valer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "#Read in HTML file\n",
    "with open('song_lyrics.html', 'r', encoding='utf-8') as html_file:\n",
    "    soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "#Extract the lyrics\n",
    "lyrics_text = soup.find('pre').get_text()\n",
    "\n",
    "#Process the lyrics to remove the \\n breaks and add periods so it doesn't interpret the entire lyric set as one sentence.\n",
    "processed_lyrics = lyrics_text.replace('\\n', '. ')\n",
    "\n",
    "#Print snippet of processed lyrics\n",
    "print(f\"Processed lyrics text:\", processed_lyrics[:100], \"...\")\n",
    "\n",
    "#Perform sentiment analysis\n",
    "blob = TextBlob(processed_lyrics)\n",
    "polarity = blob.sentiment.polarity\n",
    "\n",
    "#Create a sentiment label \n",
    "if polarity > 0:\n",
    "    sentiment_is = \"Positive\"\n",
    "elif polarity < 0:\n",
    "    sentiment_is = \"Negative\"\n",
    "else:\n",
    "    sentiment_is = \"Neutral\"\n",
    "\n",
    "#Print the sentiment polarity score and the appropriate label\n",
    "print(f\"The sentiment polarity of the lyrics is: {polarity:.2f} which is a {sentiment_is} polarity score\")\n",
    "\n",
    "#Count the number of sentences\n",
    "sentence_count = len(blob.sentences)\n",
    "\n",
    "#Print the number of sentences with an appropriate label\n",
    "print(f\"Number of sentences in the lyrics: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mTop Five Most Frequent Tokens\u001b[0m\n",
      "Token:my, Frequency: 23\n",
      "Token:'s, Frequency: 21\n",
      "Token:it, Frequency: 18\n",
      "Token:the, Frequency: 15\n",
      "Token:hide, Frequency: 15\n"
     ]
    }
   ],
   "source": [
    "#Load spaCy pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Process the lyrics with spaCy\n",
    "doc = nlp(processed_lyrics)\n",
    "\n",
    "#Filter common tokens, excluding punctuation and white space\n",
    "tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "#Count token frequencies\n",
    "token_frequencies = Counter(tokens)\n",
    "\n",
    "#Get the five most frequent tokens\n",
    "most_frequent_tokens = token_frequencies.most_common(5)\n",
    "\n",
    "#Print the five most common tokens and their frequencies\n",
    "print(\"\\033[1m\\033[32mTop Five Most Frequent Tokens\\033[0m\")\n",
    "for token, freq in most_frequent_tokens:\n",
    "    print(f\"Token:{token}, Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mMost common lemmas\u001b[0m\n",
      "Lemma: want, Frequency: 15\n",
      "Lemma: hide, Frequency: 15\n",
      "Lemma: inside, Frequency: 15\n",
      "Lemma: come, Frequency: 15\n",
      "Lemma: demon, Frequency: 15\n"
     ]
    }
   ],
   "source": [
    "#Load spaCy pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Process the lyrics with spaCy\n",
    "doc = nlp(processed_lyrics)\n",
    "\n",
    "#Filter common tokens, excluding punctuation and white space\n",
    "tokens = [token for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "#Count token frequencies\n",
    "token_frequencies = Counter(token.text.lower() for token in tokens)\n",
    "\n",
    "#Get the five most frequent tokens\n",
    "most_frequent_tokens = token_frequencies.most_common(5)\n",
    "\n",
    "#Retrieve lemmas for the most frequent tokens\n",
    "lemmas_with_frequencies = {\n",
    "    token.text.lower(): token.lemma_\n",
    "    for token in tokens\n",
    "    if token.text.lower() in dict(most_frequent_tokens)\n",
    "}\n",
    "\n",
    "#Print the common lemmas with frequencies\n",
    "print(\"\\033[1m\\033[32mMost common lemmas\\033[0m\")\n",
    "for token, lemma in lemmas_with_frequencies.items():\n",
    "    print(f\"Lemma: {lemma}, Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#Create a loop to run through the different sentences and create a score for each\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[1;32m---> 27\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mscore_sentence_by_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteresting_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     sentence_scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m, in \u001b[0;36mscore_sentence_by_tokens\u001b[1;34m(sentence, interesting_tokens)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Return the ratio\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_words \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtoken_count\u001b[49m \u001b[38;5;241m/\u001b[39m total_words\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'token_count' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Count the interesting tokens\n",
    "interesting_tokens = sum(1 for token in doc if not token.is_punct and not token.is_space)\n",
    "\n",
    "#Define scoring a sentence using tokens\n",
    "def score_sentence_by_tokens(sentence, interesting_tokens):\n",
    "    #Process the sentence with spaCy\n",
    "    doc = nlp(sentence.lower())\n",
    "\n",
    "    \n",
    "\n",
    "    #Get sentence length\n",
    "    total_words = len([token for token in doc if not token.is_punct and not token.is_space])\n",
    "\n",
    "    #Return the ratio\n",
    "    if total_words > 0:\n",
    "        return token_count / total_words\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "#Create list to store sentence scores\n",
    "sentence_scores = []\n",
    "\n",
    "#Create a loop to run through the different sentences and create a score for each\n",
    "for sent in doc.sents:\n",
    "    score = score_sentence_by_tokens(sent.text, interesting_tokens)\n",
    "    sentence_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
